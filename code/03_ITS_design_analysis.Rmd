---
title: "Interrupted time series design analysis"
author: "Jae Yeon Kim"
output:
html_document:
  toc: True
  theme: united
  number_sections: True
---

# Setup 

```{r}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # for the tidyverse framework
        patchwork, # for arranging ggplots 
        ggpubr, # for arranging ggplots 
        ggthemes, # for fancy ggplot themes
        ggsci, # pub ready themes 
        AER, # for applied econometrics
        olsrr, # for ols modeling
        outliers, # for outliers
        DMwR, # for data mining in R
        simpleboot, # for bootstrapping 
        broom, # for modeling
        plotrix, # for plotting functions
        ggfortify, # extended version of ggplot
        stargazer, # for model outputs 
        huxtable, # for model outputs 
        MASS, # for negative binomial regression
        pscl, # for zero-inflated poisson
        phenocamr, # for time series post-processing
        fANCOVA, # for nonparametric analysis of covariance
        robustbase, # for robust linear modeling 
        lmtest, # for robustness checks
        sensemakr, # for sensitivity analysis
        tm, # for text analysis 
        quanteda, # for text analysis
        xts, # for time series objects 
        zoo, # for time series objects 
        forecast, # for ARIMA modeling
        tseries, # for stat tests for time series data 
        nlme, # for GLS
        here # for reproducibility 
        )

# devtools::install_github("jaeyk/makereproducible")
library(makereproducible)

# Import R scripts

script_list <- list.files(paste0(here::here(), "/functions"),
  pattern = "*.r|*.R",
  full.names = TRUE
)

for (i in 1:length(script_list))
{
  source(script_list[[i]])
}

# for publication-friendly theme
theme_set(theme_pubr())
```


# Load files

```{r include=FALSE}

# Asian American and Latino community-based and advocacy organizations 
asian_year <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/asian_orgs.csv"))

latino_year <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/latino_orgs.csv"))

# Administrative data 
reagan <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/reagan_budget.csv"))

asian_latino_census <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/asian_latino_census.csv"))                     

reagan_funding_1982 <- read.csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/reagan_funding_1982.csv"))

medicare <- read.csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/medicare.csv"))

# Party control data 
basic_party_covariates <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/basic_party_covariates.csv"))

# Newspaper data 
ie_data <- read_csv(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/raw_data/ie_data.csv"))

agenda <- read_rds(here("processed_data", "nclr_text.rds"))

# Set preferred packages for conflicted functions 
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("annotate", "ggplot2")

```

# Wrangle data 

## Merge Asian American and Latino CBO and advocacy org 

```{r}

# Merge CBO and advocacy org data 
org_freq <- merge_by_group(asian_year, latino_year) %>% 
    count(F.year, Type, category) %>% 
    rename(Year = F.year, 
           Freq = n)

```

## Census data 

```{r}

# Rename vars 
asian_latino_census <- asian_latino_census %>%
   rename(category = Group,
          pop_percentage = Percentage)

# Subset 
asian_latino_census <- subset(asian_latino_census, category != "Total")
asian_census <- subset(asian_latino_census, category == "Asian")
latino_census <- subset(asian_latino_census, category == "Latino")

# Fill years 

all_years <- seq(min(org_freq$Year, na.rm = TRUE), 
                 max(org_freq$Year, na.rm = TRUE), by = 1) # Create year sequence 

all_years <- data.frame(Year = all_years) # Turn it into a dataframe 

## Merge year sequence with the data 

org_freq <- all_years %>%
    merge(org_freq, by = "Year", all.x = TRUE)

asian_census <- all_years %>%
    merge(asian_census, by = "Year", all.x = TRUE)

latino_census <- all_years %>%
    merge(latino_census, by = "Year", all.x = TRUE)

# Recode 

asian_census$category[is.na(asian_census$category)] <- "Asian"
latino_census$category[is.na(latino_census$category)] <- "Latino"

for (i in seq(0, 60, by = 10)){
asian_census$pop_percentage[asian_census$Year %in% c((1960 + i):((1960 + i) + 9))] <-
asian_census$pop_percentage[asian_census$Year == (1960 + i)]
}

for (i in seq(0, 60, by = 10)){
latino_census$pop_percentage[latino_census$Year %in% c((1960 + i):((1960 + i) + 9))] <-
latino_census$pop_percentage[latino_census$Year == (1960 + i)]
}

# Merge pop data 

asian_latino_census <- merge_by_group(asian_census, latino_census)

```

## Add covariates 

```{r}

reagan_org <- merge(org_freq, subset(reagan, Category == "ETES"), by = "Year")

reagan_org <- merge(reagan_org, asian_latino_census, by = c("Year", "category"))

colnames(basic_party_covariates)[1] <- "Year"

reagan_org <- merge(reagan_org, basic_party_covariates, by = "Year")

```

## Additional tidying  

```{r}

# Mutate 

reagan_org <- reagan_org %>%
    mutate(intervention = ifelse(reagan_org$Year >= 1981, 1, 0),
           intervention = factor(intervention),
           category = factor(category),
           Type = factor(Type),
           Freq = as.integer(Freq))

# Select

reagan_org <- reagan_org %>% 
   select(Year, category, Type, Freq, Percentage, pop_percentage, presidency, senate, house, intervention) 

```


# Descriptive analysis 

## Intervention 

### Overall trend 

```{r}

reagan_org %>% 
  ggplot(aes(x = Year, y = Percentage)) +
    geom_point() +
    geom_line() +   
    geom_vline(xintercept = c(1981), col = "red", linetype ="dashed") +
    annotate("rect", xmin = 1964, xmax = 1981, ymin = 0, ymax = 7.2, alpha = .2) +
    scale_y_continuous(labels = scales::percent_format(scale = 1)) +
    labs(title = "Budget for education, employment, and social service",
         y = "Percentage",
         x = "Year") +
    annotate(geom="text", x=1973, y=2, label="War on Poverty",
                color="red") +
    annotate(geom="text", x=1997, y=2, label="Post-Reagan period",
                color="red") +
    xlim(c(1964,2017))

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/reagan_budget_cut.png"))

```

### Specific programs 

```{r}

cuts <- as_tibble(reagan_funding_1982) %>%
    ggplot(aes(x = fct_reorder(Name, -Outlays), y = Outlays*0.01)) +
    geom_col() +
    coord_flip() +
    scale_y_continuous(labels = scales::percent) +
    labs(
        title = "The impact of Reagan budget cut on community empowerment programs",
        subtitle = "Percentage difference between 1981 and 1982",
        x = "Grant names", 
        y = "Percentage")

survived <- medicare %>%
    ggplot(aes(x = factor(year), y = amount)) +
    geom_vline(xintercept = factor(1981)) +
    geom_col() +
    labs(
       title = "The impact of Reagan budget cut on indiviual assistance programs",
       subtitle = "Total Medicare spending from 1970 to 2018",
        x = "Year",
        y = "Amount"
    ) + 
    scale_y_continuous(labels = scales::unit_format(unit = "Millions"))

cuts / survived 

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/grant_changes.png"), width = 12, height = 8)

```

### Newspaper analysis 

#### International Examiner 

  - N = 3,120

```{r}

ie_data$reagan <- str_detect(ie_data$text, "reagan|Reagan")

ie_reagan <- ie_data %>% filter(reagan == TRUE)
  
ie_dic <- dfm(corpus(ie_reagan$text), 
              dictionary = dictionary(list(budget = 
              c("spending","grants","grant","funding","funds","fund"))))

# Add to the original data  
ie_reagan$budget <- convert(ie_dic, to = "data.frame")[,2]

# Plot
ie_reagan_n <- ie_reagan %>%
    mutate(year = as.numeric(year)) %>%
    group_by(year) %>% 
    summarise(n = n(),
              count = sum(budget))

# Fill in 0 years
all_years <- seq(min(ie_reagan$year), max(ie_reagan$year))

# Turn it into a dataframe 
all_years <- data.frame(year = all_years)
  
# Count by year and type
ie_reagan_n <- ie_reagan_n %>% right_join(all_years, by = "year")

# Add missing rows
ie_missing_years <- c(1976:1982)[!(c(1976:1982) %in% unique(ie_reagan_n$year))]

ie_reagan_n <- ie_reagan_n %>%
  add_row(year = ie_missing_years,
          n = rep(0, length(ie_missing_years)),
          count = rep(0, length(ie_missing_years)))

# Filter 
ie_reagan_n_filtered <- filter(ie_reagan_n, between(year, 1976, 1981))
          
# No NAs
ie_reagan_n_filtered <- ie_reagan_n_filtered[!is.na(ie_reagan_n_filtered$count), ]

# Plot
ie_reagan_plot <- ggplot(ie_reagan_n_filtered, aes(x = year)) +
    geom_col(aes(y = n), alpha = 0.5) +
    geom_point(aes(y = count), col = "black") +
    geom_line(aes(y = count), col = "red") +
    labs(x = "Year", y = "Count",
         title = "International Examiner") 

```

#### Agenda 

  - N: 1,249

```{r}

# Create a year variable 

agenda$year <- str_sub(agenda$date, 1, 4)

# Create a document term matrix based on a custom dictionary 

# Set a condition and use it to filter the data 
agenda$reagan <- str_detect(agenda$value, "reagan|Reagan")

agenda_reagan <- agenda %>% filter(reagan == TRUE)
  
agenda_dic <- dfm(corpus(agenda_reagan$value), 
              dictionary = dictionary(list(budget = 
              c("spending","grants","grant","funding","funds","fund"))))

# Add to the original data  
agenda_reagan$budget <- convert(agenda_dic, to = "data.frame")[,2]

# Plot
agenda_reagan_n <- agenda_reagan %>%
    mutate(year = as.numeric(year)) %>%
    group_by(year) %>% 
    summarise(n = n(),
              count = sum(budget))

# Fill in 0 years
all_years <- seq(min(agenda_reagan$year), max(agenda_reagan$year))

# Turn it into a dataframe 
all_years <- data.frame(year = all_years)
  
# Count by year and type
agenda_reagan_n <- agenda_reagan_n %>% right_join(all_years, by = "year")

# Add missing rows 
agenda_missing_years <- c(1976:1982)[!(c(1976:1982) %in% unique(agenda_reagan_n$year))]

agenda_reagan_n <- agenda_reagan_n %>%
  add_row(year = agenda_missing_years,
          n = rep(0, length(agenda_missing_years)),
          count = rep(0, length(agenda_missing_years)))

agenda_reagan_n[is.na(agenda_reagan_n)] <- 0

# Filter 
agenda_reagan_n_filtered <- filter(agenda_reagan_n, between(year, 1976, 1981))
          
# No NAs
agenda_reagan_n_filtered <- agenda_reagan_n_filtered[!is.na(agenda_reagan_n_filtered$count), ]

# Plot

agenda_reagan_plot <- ggplot(agenda_reagan_n_filtered, aes(x = year)) +
    geom_col(aes(y = n), alpha = 0.5) +
    geom_point(aes(y = count), col = "black") +
    geom_line(aes(y = count), col = "red") +
    labs(x = "Year", y = "Count",
         title = "Agenda (NCLR newsletter)")

ie_reagan_plot + agenda_reagan_plot

ggsave(here("outputs", "dic_analysis.png"),
       height = 5)
```
# Treating outliers 

```{r}

model <- lm(log(Freq) ~ Percentage + pop_percentage + category + presidency + senate + house, data = reagan_org)
  
# Detecting outliers 

cooksd <- cooks.distance(model)

# For reproducibility 
set.seed(1234)

influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm = TRUE))])

# Visualizing outliers

cooksd_df <- data.frame(Index = seq(1:220),
                        CookD = cooksd)

cooksd_df %>%
  ggplot(aes(x = Index, y = CookD)) +
  geom_col() +
  labs(x = "Index", y = "Cook's Distance") +
  geom_hline(yintercept = 4*mean(cooksd, na.rm=T), col="red",
             linetype = "dashed") +
  annotate(geom = "text", x = 100, y = 0.05, label="Threshold: 0.02", color="red")

ggsave("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/outliers_detected.png")

```

```{r}
# Replace with NAs

reagan_org$Freq[influential] <- NA

# Imputation  

knnOutput <- knnImputation(reagan_org %>% 
             mutate(category = factor(category),
                    Type = factor(Type)))

reagan_org$Freq <- round(knnOutput$Freq, 0)

reagan_org$Freq <- as.integer(reagan_org$Freq)
```

# Statistical modeling 


### AIC in time 

```{r}

# Initialization vars 
lm.AIC <-NA
lm_log.AIC <-NA
poisson.AIC <- NA
nb.AIC <- NA
year <-NA

# For loop 
for (i in c(0:38)){

# Models 
lm.out <- lm(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))

lm_log.out <- lm(log(Freq) ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))
  
poisson.out <- glm(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i), family = "poisson")

nb.out <- glm.nb(Freq ~ Percentage + pop_percentage + category, data = subset(reagan_org, Year <= 1970 + i))

# AIC scores of the models 
lm.AIC[i] <- AIC(lm.out)
lm_log.AIC[i] <- AIC(lm_log.out)
poisson.AIC[i] <- AIC(poisson.out)
nb.AIC[i] <- AIC(nb.out)

# Year var 
year[i] <- 1970 + i
}

# Putting them together as a dataframe 
AICs <- data.frame("OLS" = lm.AIC,
                   "Logged OLS" = lm_log.AIC,
                   "Poisson" = poisson.AIC,
                   "Negative_binomial" = nb.AIC,
                   "Year" = year)

# Visualization
AICs %>%
  gather(Models, values, OLS:Negative_binomial) %>%
  ggplot(aes(x = Year, y = values, col = Models)) +
  scale_color_brewer(type = "div", palette = "Set1") +
  geom_point() +
  geom_line() +
  labs(y = "AIC scores") +
  scale_color_npg()
  

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/AIC_in_time.png"),
              height = 6, width = 10)

```

## Model fits 

```{r}

# Reset data for modeling 
reagan_org$Type[reagan_org$Type == "Hybrid"] <- "CBO"

reagan_org_fit <- reagan_org %>% filter(between(Year, 1960, 1990))

```

```{r}
ols.model <- ols_its(reagan_org_fit) + ggtitle("OLS") + labs(y = "Organizational founding") 

logged.ols.model <- ols_its(reagan_org_fit %>% mutate(Freq = log(Freq))) + ggtitle("OLS with logged DV") + labs(y = "Organizational founding ") 
  
poisson.model <- ps_its(reagan_org_fit) + ggtitle("Poisson") 

nb.model <- nb_its(reagan_org_fit) + ggtitle("Negative binomial") 

ggarrange(ols.model, logged.ols.model, poisson.model, nb.model)

ggsave("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/pred_plots.png", height = 10, width = 10)

```

### Heterogeneous treatment effects 

```{r}
pred_org_type <- ols_its(reagan_org_fit %>% mutate(Freq = log(Freq))) + 
  labs(title = "How treatment effects vary by organizational types",
       y = "Logged organizational founding") + 
  facet_grid(~Type) +
  scale_y_continuous(breaks = scales::pretty_breaks())

pred_org_type

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/pred_org_type.png"))

reagan_org_fit_cbo <- subset(reagan_org_fit, Type == "CBO")

pred_org_group <- ols_its_only_cbo(reagan_org_fit_cbo %>% mutate(Freq = log(Freq))) + 
  labs(title = "How treatment effects vary by groups",
       subtitle = "Only included CBOs",
       y = "Logged organizational founding") + 
  facet_wrap(~category)

pred_org_group 

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/pred_org_group.png"))

pred_org_type / pred_org_group

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/pred_subgroups.png"), height = 7, width = 6)

```
## Model evaluations

\begin{table}[!htbp] \centering 
  \caption{ITS design analysis results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{Founding rate (logged)} \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 Year & 0.053$^{**}$ & 0.057$^{**}$ \\ 
  & (0.022) & (0.022) \\ 
  & & \\ 
 intervention1 & $-$68.525 & $-$70.179 \\ 
  & (91.630) & (91.119) \\ 
  & & \\ 
 categoryLatino &  & 0.213 \\ 
  &  & (0.157) \\ 
  & & \\ 
 Year:intervention1 & 0.034 & 0.035 \\ 
  & (0.046) & (0.046) \\ 
  & & \\ 
 Constant & $-$104.474$^{**}$ & $-$110.847$^{**}$ \\ 
  & (42.642) & (42.659) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 78 & 78 \\ 
R$^{2}$ & 0.128 & 0.149 \\ 
Akaike Inf. Crit. & 169.575 & 169.627 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}  

- Without group category variable 
  - p-value for Intervention: 0.4569254
  - p-value for Intervention:Year: 0.4617835

- With group category variable 
  - p-value for Intervention: 0.4569254
  - p-value for the interaction between Intervention and Year: 0.4617835

```{r}

lm.out <- lm(Freq ~ Year + intervention + Year*intervention + category, data = reagan_org_fit_cbo)

lm_log.out <- lm(log(Freq) ~ Year + intervention + Year*intervention + category, data = reagan_org_fit_cbo)

lm_log.out1 <- lm(log(Freq) ~ Year + intervention + Year*intervention + category, data = reagan_org_fit_cbo)

lm_log.out2 <- lm(log(Freq) ~ Year + intervention + Year*intervention, data = reagan_org_fit_cbo)
  
poisson.out <- glm(Freq ~ Year + intervention + Year*intervention + category, data = reagan_org_fit_cbo, family = "poisson")

nb.out <- glm.nb(Freq ~ Year + intervention + Year*intervention + category, data = reagan_org_fit_cbo)

stargazer(add_AIC_pvalue(lm_log.out2), 
          add_AIC_pvalue(lm_log.out1), 
          title = "ITS design analysis results", 
          header = FALSE, 
          dep.var.labels = c("Founding rate (logged)", "Founding rate (logged)"),
          keep.stat = c("AIC", "rsq", "n"))

add_AIC_pvalue(lm_log.out2)$p.value[3:4]

add_AIC_pvalue(lm_log.out1)$p.value[3:4]

```

#### Overdispersion test

- Alpha is positive. Dispersion present.

Overdispersion test

data:  poisson.out
z = 1.9355, p-value = 0.02647
alternative hypothesis: true alpha is greater than 0
sample estimates:
    alpha 
0.4554112 

```{r}

dispersiontest(poisson.out, trafo = 1)

```

#### AIC 

OLS AIC:360.487
Logged OLS AIC:169.627
Poisson AIC:335.942
Negative binomial AIC:330.525

```{r}
cat(paste0("OLS AIC:", round(AIC(lm.out), 3), "\n",   
           "Logged OLS AIC:", round(AIC(lm_log.out), 3), "\n",
           "Poisson AIC:", round(AIC(poisson.out), 3), "\n",
           "Negative binomial AIC:", round(AIC(nb.out), 3)))
```

## Discontinuity in time

### Coefficients 

```{r}

# Initilization vars 
fed <- NA
pop <- NA
year <- NA

# For loop 
for (i in c(0:20)){

# Model   
model <- lm(log(Freq) ~ Percentage + pop_percentage + category, data = subset(reagan_org_fit_cbo, Year <= 1970 + i))

# Save iterations 
fed[i] <- model$coefficients[3] %>% as.numeric()
pop[i] <- model$coefficients[4] %>% as.numeric()
year[i] <- 1970 + i}

# Putting them as a data frame 
for_loop <- data.frame("Fed_funding" = fed,
                       "Pop_growth" = pop,
                       "Year" = year)

# Visualization
fed_coef <- for_loop %>%
  ggplot(aes(x = Year, y = Fed_funding)) +
  geom_point() +
  geom_vline(xintercept = c(1980), linetype = "dashed", size = 1, color = "red") +
  labs(title = "Federal funding", 
       subtitle = "Only included CBOs", y = "OLS coefficients") 

pop_coef <- for_loop %>%
  ggplot(aes(x = Year, y = Pop_growth)) +
  geom_point() +
  geom_vline(xintercept = c(1980), linetype = "dashed", size = 1, color = "red") +
  labs(title = "Population growth", 
       subtitle = "Only included CBOs", y = "OLS coefficients") 

fed_coef + pop_coef 

ggsave("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/reg_coeffs.png",
              height = 6, width = 10)

```

### Bootstrapping 

```{r}

boot_ci <- function(i){

# For reproducibility 
set.seed(1234)

# Init vars 
stat = data.frame()
result = data.frame()
year = data.frame()

# Model 
m = Boot(lm(log(Freq) ~ Percentage + pop_percentage + category, data = subset(reagan_org_fit_cbo, Year <= 1970 + i)), R = 1000)

# Extract bootSE
stat = rbind(stat, summary(m) %>% data.frame() %>% select(bootSE))

# Putting them together as a dataframe 
Year = rep(c(1970 + i), nrow(stat))

Names = c("Intercept", "Fed_SE", "Pop_SE", "Latino_SE")

result = cbind(stat, Year, Names)

rownames(result) <- NULL

result}

```

```{r}

cis <- data.frame()

for (i in c(1:20)){
  print(paste(i, "iteration completed"))
  cis = rbind(cis, boot_ci(i))
}

colnames(cis)[1:2] <- c("SE", "Year")

cis_spread <- cis %>%
  spread(Names, SE) %>%
  select(-c(Intercept, Latino_SE))

boot_merged <- merge(cis_spread, for_loop)

```

```{r}
fed_boot <- boot_merged %>%
  ggplot(aes(x = Year, y = Fed_funding, 
             ymax = Fed_funding + 2*Fed_SE, 
             ymin = Fed_funding - 2*Fed_SE)) +
  geom_pointrange() +
  labs(x = "Year", y = "OLS coefficients", 
       title = "Federal funding",
       subtitle = "With bootstrapped CIs") +
  geom_vline(xintercept = c(1980), linetype = "dashed", size = 1, color = "red") +  
  annotate("rect", xmin = 1970, xmax = 1990, ymin = -0.5, ymax = 0, alpha = .2)
  
pop_boot <- boot_merged %>%
  ggplot(aes(x = Year, y = Pop_growth, 
             ymax = Pop_growth + 2*Pop_SE, 
             ymin = Pop_growth - 2*Pop_SE)) +
  geom_pointrange() +
  labs(x = "Year", y = "OLS coefficients", 
       title = "Population growth",
       subtitle = "With bootstrapped CIs") +
  geom_vline(xintercept = c(1980), linetype = "dashed", size = 1, color = "red") +
  annotate("rect", xmin = 1970, xmax = 1990, ymin = -2, ymax = 0, alpha = .2)

fed_boot + pop_boot

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/boot_cis.png"), 
       height = 6, width = 10)

```

## Standard errors 

I adated some code from [this post](https://www.brodrigues.co/blog/2018-07-08-rob_stderr/).

```{r}

lm_log.out <- lm(log(Freq) ~ Percentage + pop_percentage + category + presidency + senate + house, data = subset(reagan_org_fit_cbo, Year <= 1980))

```

### Heteroskedasticity 

- Null: the variance of residuals is constant (homoscedasticity).

studentized Breusch-Pagan test

data:  lm_log.out
BP = 9.9427, df = 4, p-value = 0.0414

```{r}

lmtest::bptest(lm_log.out, studentize = TRUE)

```

### Autocorrelation test 

- Null: the autocorrelation of residuals is 0. 

Durbin-Watson test

data:  lm_log.out
DW = 2.3122, p-value = 1
alternative hypothesis: true autocorrelation is greater than 0

```{r}

lmtest::dwtest(lm_log.out)

```

- ACF shows an autoregressive pattern. 

```{r}

ggAcf(residuals(lm_log.out))

ggsave(make_here("/home/jae/analyzing-asian-american-latino-civic-infrastructure/outputs/acf_test.png"))

```

- Using HAC standard errors 

```{r}
# Coefficient summary using the Newey-West SE estimates
lm_new <- coeftest(lm_log.out, vcov = NeweyWest, prewhite = F, adjust = T)
  
```

### Outliers 

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & log(Freq) &   & log(Freq) \\ 
\\[-1.8ex] & \textit{OLS} & \textit{coefficient} & \textit{MM-type} \\ 
 & \textit{} & \textit{test} & \textit{linear} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\hline \\[-1.8ex] 
 Percentage & 0.293$^{***}$ & 0.293$^{***}$ & 0.295$^{***}$ \\ 
  & (0.107) & (0.075) & (0.076) \\ 
  & & & \\ 
 pop\_percentage & $-$0.028 & $-$0.028 & $-$0.035 \\ 
  & (0.159) & (0.101) & (0.124) \\ 
  & & & \\ 
 categoryLatino & 0.484 & 0.484 & 0.525 \\ 
  & (0.557) & (0.367) & (0.456) \\ 
  & & & \\ 
 presidency & $-$0.020 & $-$0.020 & $-$0.012 \\ 
  & (0.189) & (0.171) & (0.197) \\ 
  & & & \\ 
 senate &  &  &  \\ 
  &  &  &  \\ 
  & & & \\ 
 house &  &  &  \\ 
  &  &  &  \\ 
  & & & \\ 
 Constant & $-$0.551 & $-$0.551$^{*}$ & $-$0.560$^{*}$ \\ 
  & (0.425) & (0.286) & (0.299) \\ 
  & & & \\ 
\hline \\[-1.8ex] 
Observations & 46 &  & 46 \\ 
R$^{2}$ & 0.259 &  & 0.250 \\ 
Adjusted R$^{2}$ & 0.187 &  & 0.177 \\ 
Residual Std. Error (df = 41) & 0.631 &  & 0.685 \\ 
F Statistic & 3.589$^{**}$ (df = 4; 41) &  &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

```{r include=FALSE}

lmrobfit <- lmrob(log(Freq) ~ Percentage + pop_percentage + category + presidency + senate + house, data = subset(reagan_org_fit_cbo, Year <= 1980))

stargazer(lm_log.out, lm_new, lmrobfit,
          header = FALSE, 
          keep.stat = c("rsq", "n"))

```

## Sensitivity analysis 

Sensitivity Analysis to Unobserved Confounding

Model Formula: log(Freq) ~ Percentage + pop_percentage + category + presidency + 
    senate + house

Null hypothesis: q = 1 and reduce = TRUE 
-- This means we are considering biases that reduce the absolute value of the current estimate.
-- The null hypothesis deemed problematic is H0:tau = 0 

Unadjusted Estimates of 'Percentage': 
  Coef. estimate: 0.2935 
  Standard Error: 0.1069 
  t-value (H0:tau = 0): 2.7451 

Sensitivity Statistics:
  Partial R2 of treatment with outcome: 0.1553 
  Robustness Value, q = 1: 0.3465 
  Robustness Value, q = 1, alpha = 0.05: 0.1034 

Verbal interpretation of sensitivity statistics:

-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 15.53% of the residual variance of the treatment to fully account for the observed estimated effect.

-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 34.65% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 34.65% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.

-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 10.34% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 10.34% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.

```{r}

lm.sense <- sensemakr(lm_log.out, treatment = "Percentage")
     
summary(lm.sense)

```